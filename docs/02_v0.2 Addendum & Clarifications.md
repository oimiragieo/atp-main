# v0.2 Addendum & Clarifications (Addressing Open Questions)

This addendum specifies concrete mechanisms for estimation accuracy, consensus semantics, router scalability/state, backpressure, and tool/environment integration. It also introduces new control payloads, scoring metrics, and policy fields.

## A) The Estimation Oracle Problem

### A.1 Estimator Contract (ProviderAdapter)

**New signature**

```python
class ProviderAdapter:
    def estimate(self, prompt: dict) -> dict:
        """Return conservative cost/latency estimates.
        Returns:
          {
            "tokens_in": int,           # mean
            "tokens_out": int,          # mean
            "usd": float,               # mean
            "p95_tokens": int,          # optional
            "p95_usd": float,           # optional
            "variance": {"tokens": float, "usd": float},
            "confidence": float,        # 0..1
            "tool_cost_breakdown": [{"tool":"search_api","usd":0.02}],
            "assumptions": ["ctx<=120k","no_tool_loop>2"]
          }
        """
```

**Router rule:** use **guard‑banded** estimate: `eff_usd = mean_usd + k * sqrt(variance_usd)`, where `k` is QoS‑dependent (e.g., gold=2.0, silver=1.0, bronze=0.5). Same for tokens.

### A.2 Estimation Accuracy Telemetry

Add to FIB per agent:

```yaml
fib_runtime:
  reviewer.gemini:
    estimate_mape_7d: 0.18      # mean absolute pct error vs. actual usd
    underestimate_rate_7d: 0.12 # fraction of requests where actual>estimate by >20%
    p95_overage_factor: 1.35
```

**Scoring penalty:**

```
predictability = 1 / (1 + estimate_mape_7d)
under_penalty  = 1 - min(underestimate_rate_7d, 0.5)
score = ... + w_pred * predictability + w_safety * under_penalty
```

Agents with poor estimation are de‑prioritized; repeated underestimation can **downgrade QoS** tier.

### A.3 Anti‑Thrash Controls

* **Min window hold**: after WINDOW\_UPDATE, hold for `τ_min` (e.g., 500ms) before shrinking/increasing again.
* **Burst cap**: limit per‑agent new in‑flight sends per second.
* **Cool‑off on underestimation**: if actual>eff by >X%, halve parallelism and add temporary surcharge to that agent’s effective cost.

---

## B) Consensus Complexity

### B.1 Finding Canonicalization

All agent outputs MUST map to a **Finding Schema** to enable semantic comparison:

```jsonc
{
  "id": "F-…",
  "type": "code.vuln.aud_check_missing",  // taxonomy key
  "file": "auth/jwt.py",
  "span": "45-80",
  "claim": "Missing audience check on JWT decode",
  "evidence": [ {"kind":"code","file":"auth/jwt.py","lines":"60-72"} ],
  "proposed_fix": "…",
  "tests": ["test_jwt_aud"],
  "severity": "high",
  "confidence": 0.83
}
```

Router provides a tiny **canonicalizer** that normalizes synonyms onto a shared ontology (configurable YAML). Example:

```yaml
aliases:
  - match: ["audience claim missing","no aud check","missing aud"]
    to: code.vuln.aud_check_missing
```

### B.2 Semantic Agreement Scorer

* **Embedding similarity**: compute cosine similarity between claims after canonicalization using a **local embedding model** (e.g., all‑MiniLM/Llama embed). Aggregate across findings targeting the same file/span/type.
* **Evidence overlap**: Jaccard on file\:line ranges, not text.
* **Structured fields** (type/severity/tests) must match to count as agreement.
  Agreement score:

```
agreement = 0.5*mean(cosine(embeddings)) + 0.3*evidence_overlap + 0.2*field_match
```

### B.3 Provisional Consensus (Streaming)

* Router computes **provisional results** once any subset reaches `quorum_min` (e.g., 2 agents or agreement≥0.6) and **latency budget** is tight.
* Mark frame `payload.type = "agent.result.provisional"` and include `expiry_ms`. If later frames reduce confidence below threshold, router revises.
* Final consensus waits for either: (a) **all FINALs**, (b) **deadline**, or (c) **confidence≥τ\_final**.

### B.4 Arbiter LLM (Optional)

* Lightweight **arbiter** (local or mid‑tier) can rewrite divergent findings into a single structured list **without** new claims, only reconciliation. Budget‑guarded via `arbiter_max_usd`.

---

## C) Router Scalability & State Management

### C.1 External State Store

Persist per‑stream state in **Redis** (or KeyDB/Memcached for hot path) with TTL heartbeat:

```yaml
stream_state:
  session_id: …
  stream_id: …
  next_msg_seq: 13
  window: {parallel:2,tokens:80000,usd:0.5}
  budget_left: {tokens:220k,usd:0.92}
  reassembly_index: { "12": {"missing_frags": [1,2]}, … }
  consensus_buffer_hash: "sha256:…"
  last_heartbeat_ts: 1734550000
```

**WAL**: Append all frames to **Kafka/Redpanda** topic `atp.frames` for replay. On crash, a new router instance rehydrates from Redis + WAL and resumes using idempotency keys.

### C.2 Horizontal Scaling

* **Sharding**: Rendezvous hashing on `stream_id` → router shard. Sticky sessions via L4 (ingress) or application layer redirect.
* **Multi‑router topology**: introduce an **Agent Gateway Protocol (AGP)** where routers exchange **ROUTE\_UPDATE** messages containing reachability + metrics of local agents. Fields:

```jsonc
{
  "router_id":"rtr‑east‑1a",
  "agent_prefix":"reviewer.*",
  "caps":{"qos":["gold"],"max_parallel":64},
  "metrics":{"p95_ms":1100,"err_rate":0.02}
}
```

* **Federation**: clients connect to a regional router; inter‑router hops forward frames (like BGP + MPLS labels), preserving `session_id` and budgets with **per‑hop decrements**.

### C.3 Resilience

* **Leaderless** per‑stream (shard owner is implicit via hashing).
* **Dead‑man switch**: if no heartbeat from router shard, **lease expires** and a standby claims the shard range.
* **Resumption token**: client may send `resume_token` to re‑attach; duplicates are deduped by `msg_seq`.

---

## D) Backpressure (Agent→Router)

### D.1 CTRL/STATUS Payload

Agents can signal load and request reduced windows.

```jsonc
{
  "type": "control.status",
  "status": "READY|BUSY|PAUSE|DRAINING",
  "reason": "gpu_oom|queue_depth|maintenance",
  "metrics": {"queue_depth": 42, "gpu_util": 0.94, "mem_free_mb": 512},
  "suggested_window": {"max_parallel": 0, "max_tokens": 0, "max_usd": 0},
  "retry_after_ms": 120000
}
```

**Effective window** = `min(router_window, agent_suggested_window)` component‑wise. Router **MUST** honor `PAUSE` for that agent within `grace_ms` (policy‑bound), except for gold QoS with explicit override.

### D.2 NACK Codes

* `EWINDOW_AGENT` — agent refused due to overload; includes `retry_after_ms`.
* `EDRAINING` — finishing in‑flight, not accepting new.

---

## E) Tools & L0 Environment Integration

### E.1 Tool Budgeting

* `estimate()` **MUST** include `tool_cost_breakdown` and `tool_token_estimates` (if tools are LLMs).
* Router debits tool cost from **stream budget**; per‑tool sub‑budgets can be configured in policy:

```yaml
policy:
  tools_budget_caps:
    search_api: {usd: 0.10}
    code_index: {usd: 0.05}
```

### E.2 Tool Permissions & Routing

Add to `meta`:

```jsonc
{
  "tool_permissions": ["fs.read","fs.write","net.egress:https","secrets:kv:read"],
  "environment_id": "sandbox‑fs‑1",
  "security_groups": ["sandboxed-fs","no-pii"]
}
```

Policies can require capabilities:

```yaml
- match: { task_type: code_review, requires: ["fs.read"] }
  fanout: [summarizer.local]
  constraints: { security_groups_any: ["sandboxed-fs"] }
```

### E.3 Tool Control Messages

Define L0 interop frames:

```jsonc
{"type":"tool.request","tool":"fs.read","args":{"path":"/repo/auth/jwt.py"},"budget_usd":0.0}
{"type":"tool.result","ok":true,"data":"…","cost_usd":0.0}
```

Router may mediate tool access or pass‑through depending on deployment mode.

---

## F) Schema & Spec Diffs

### F.1 New/Updated Fields

* Frame.payload.type += `"agent.result.provisional"`, `"control.status"`, `"tool.request"`, `"tool.result"`.
* Add `payload.expiry_ms` for provisional results.
* Add `meta.tool_permissions`, `meta.environment_id`, `meta.security_groups`.
* Add `payload.tool_cost_breakdown` in estimates.

### F.2 JSON Schema Versioning

Increment schema minor version to **v1.1**; maintain wire compatibility with v1.0 (unknown types ignored).

---

## G) Policy & Scoring Updates (Copy‑Paste)

```yaml
weights:
  w_capability: 0.30
  w_perf: 0.20
  w_cost: 0.15
  w_latency: 0.10
  w_context: 0.10
  w_pred: 0.10           # new (estimation predictability)
  w_safety: 0.05         # new (underestimation penalty)

fib_runtime:
  summarizer.local:
    estimate_mape_7d: 0.08
    underestimate_rate_7d: 0.03

policies:
  - match: { task_type: code_review, diff_loc: ">200" }
    fanout: [summarizer.local, reviewer.gemini]
    arbiter_max_usd: 0.05
    provisional: { quorum_min: 1, agree_min: 0.6, expiry_ms: 15000 }
    budget: { usd: 3.00, tokens: 1_200_000 }
```

---

## H) Reference Router Changes

* Ingress: accept `control.status` and update **Agent Registry** with `suggested_window`, `status`, `retry_after_ms`.
* Dispatcher: **effective window** uses `min(router_window, agent_window)`; skip BUSY/PAUSE agents.
* State: persist `window_state`, `backpressure_state`, `consensus_buffer` in Redis; append all frames to WAL.
* Consensus: add **canonicalizer** + embedding‑based **agreement scorer**; enable **provisional** mode.
* Budget Gov: guard‑band on variance; cool‑off penalty on underestimation.

---

## I) Open Questions for v0.3

1. **Inter‑router security** for AGP (mutual attestation, rate‑limited route leaks).
2. **Formal ontology** of findings beyond code review (retrieval, planning, data agents).
3. **Fairness across tenants**: per‑tenant token/\$ quotas with hierarchical scheduling.
4. **Privacy**: differential privacy knobs for traces/telemetry.

---

## J) Migration Notes

* All v0.1 adapters remain compatible; implement `estimate()` extended fields progressively.
* Routers without embedding models can keep lexical agreement; enable semantic scorer when available.

*End of v0.2 addendum.*
